{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2106628",
   "metadata": {},
   "source": [
    "# Machine Learning Lifecycle I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72527fa-d111-4c6e-96fa-0ac4eec74edb",
   "metadata": {},
   "source": [
    "The lifecycle is the process of developing, deploying, and maintaining a machine learning model for a specific application. This shall serve as our fundamental guideline as to how we are to approach in creating the solutions required for our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c5e00-035f-4cdf-875b-b7c78b39a1fb",
   "metadata": {},
   "source": [
    "Below are the first three comprehensive steps laid out in the lifecycle in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026b5ea-b577-4ead-b0a2-431325f82063",
   "metadata": {},
   "source": [
    "## Problem Identification and Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6b344-1364-4b37-8616-aa2e91d0e6b4",
   "metadata": {},
   "source": [
    "The first step in the lifecycle and is crucial as it sets the direction for the project, and involves defining the problem and create the objectives necessary for the resolution of the problem defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fafbaf-0b9f-47cc-a05f-c8413f6b4d64",
   "metadata": {},
   "source": [
    "**Define the Problem**: The first task in this stage is to clearly define the problem. This involves understanding what the we want to achieve and how a machine learning model can assist in achieving that goal. For this instance, this module aims to guide us together towards the creation of a model that will recognize the hand signs shown in the camera based from the American Sign Language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560b302-0fa3-429f-a9b3-ac461bbcbc4e",
   "metadata": {},
   "source": [
    "**Determine the Machine Learning Task**: Once the problem is defined, it's necessary to define the machine learning task based on that problem. This could involve deciding whether the problem is a classification problem, a regression problem, a clustering problem, etc. For our case, the task at hand would be a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529b91c-6961-4978-9330-9676c8d30a3a",
   "metadata": {},
   "source": [
    "**Define the Optimization Objective**: The next step would be to determine the key business performance metrics that the machine learning model should aim to improve. These metrics should align with the overall objectives. For our use case, if the goal is to ensure an accurate recognition of the hand sign shown in the picture, the optimization objective might be to reduce the error rate of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae69149-2c82-47a1-ad12-61ef7a1d810a",
   "metadata": {},
   "source": [
    "**Review Data Requirements**: Reviewing data requirements involves determining what data is needed to solve the problem and whether the necessary data is available. This might also involve considering the cost of data acquisition and whether external data sources might improve model performance. For our case, we will need to collect images of hand signs based in the American Sign Language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980fbb7-5de3-4817-99d9-d7cec18386db",
   "metadata": {},
   "source": [
    "## Data Collection and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5243b6e-f963-49a6-8d38-dda346044e3e",
   "metadata": {},
   "source": [
    "This step in the lifecycle involves the gathering of the necessary data for training the model and understanding its characteristics. Personally, this stage in the lifecycle is the most expensive and time-consuming depending on the data needed. For our case, the American Sign Language datasets are common and free versions can be acquired from the Internet for use of everybody so we will not encounter much problem on this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c1828-934e-42f5-a582-295d977555e0",
   "metadata": {},
   "source": [
    "**Data Colection**: During the data collection, data relevant to the problem at hand is gathered. The data may come from various sources, such as databases, APIs, web scraping, third-party providers, or even generated through experiments or surveys. The collected data should contain the features necessary to solve the problem and the target variable for supervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c860-7628-4e49-9cfb-2c448613e167",
   "metadata": {},
   "source": [
    "**Data Exploration**: Once the data is collected, it needs to be explored to understand its characteristics. This is often done using descriptive statistics and data visualization techniques. This stage helps identify patterns, relationships, or anomalies in the data. During this stage, it's also important to understand the context of the data: where it comes from, how it was collected, what each feature represents, etc. This can provide valuable insights that can guide the next stages of the lifecycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cef257-7ef2-4036-974b-23f3388b7609",
   "metadata": {},
   "source": [
    "**Importance of Data Exploration**: Data exploration can help identify potential issues with the data, such as missing or inconsistent values, outliers, or imbalanced classes. It can also help in understanding the distribution of the data, the relationship between variables, or the presence of subgroups in the data. The insights gained from the data exploration stage can inform the subsequent Data Processing and Feature Engineering stage, where the raw data is cleaned and transformed into a form suitable for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a798375-c66e-4812-9aeb-45fbc5c4d7db",
   "metadata": {},
   "source": [
    "## Data Processing/Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f8849-897e-4593-9c9e-bfaa0ddcfaa2",
   "metadata": {},
   "source": [
    "Data processing or feature engineering is an essential step in the machine learning lifecycle as it involves transforming the raw data into a format that is suitable for training machine learning models and extracting meaningful features from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873dc64-adfb-4880-8155-c6b8bb5ade20",
   "metadata": {},
   "source": [
    "A special note would be for feature engineering as it involves creating new features or transforming existing features to better represent the underlying patterns in the data. It requires domain knowledge and a deep understanding of the problem at hand. The goal is to extract relevant information from the raw data that can improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01027147-f565-45bd-886f-11072c4bd400",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9756a64-9e28-43c4-b33b-e4ee166e7cdf",
   "metadata": {},
   "source": [
    "**Data Cleaning**: This step involves handling missing values, outliers, and inconsistencies in the data. Missing values can be imputed using techniques such as mean imputation, median imputation, or using advanced imputation methods like K-nearest neighbors or regression imputation. Outliers can be detected and treated by methods like Z-score, IQR, or using domain knowledge to determine if they are valid or erroneous. Inconsistent data can be resolved by standardizing units, resolving conflicting values, or correcting errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e253b5-6526-4c91-8cf0-9554781f0ff7",
   "metadata": {},
   "source": [
    "**Data Splitting**: The collected data is typically split into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters and evaluate different model variations, and the test set is used to assess the final model's performance. The splitting ratio depends on the size of the dataset and the specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cafb027-2145-4010-83f4-f7bcd041a0b1",
   "metadata": {},
   "source": [
    "**Data Normalization and Scaling**: It is common to normalize or scale the data to ensure that different features have similar scales. This helps prevent certain features from dominating the learning process due to their larger magnitude. Techniques like min-max scaling, z-score standardization, or logarithmic scaling can be used depending on the distribution and characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640b559-8a2d-4b51-a4a8-e28ed2026cc7",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20607d53-27c7-4160-9b49-8199b6439e88",
   "metadata": {},
   "source": [
    "**Feature Extraction**: This involves extracting relevant information from the raw data. For example, extracting the day of the week from a timestamp, extracting keywords from text data, or extracting color histograms from images. Feature extraction can be done using methods like Principal Component Analysis (PCA), Fourier Transform, or using domain-specific algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278792c1-f8ca-473d-b70d-38a9fb0acf9c",
   "metadata": {},
   "source": [
    "**Feature Transformation**: This involves transforming the data to meet certain assumptions or improve its distribution. Common transformations include logarithmic transformations, square root transformations, or Box-Cox transformations. These transformations can help normalize the data, reduce skewness, or make it more suitable for linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeaf38c-fd0c-4029-a64e-2c183fac08b0",
   "metadata": {},
   "source": [
    "**Feature Selection**: Feature selection aims to identify the most relevant features that contribute the most to the model's predictive power. This helps reduce dimensionality and improve model interpretability. Techniques like correlation analysis, forward/backward selection, or regularization methods like L1/L2 regularization can be used for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3782da",
   "metadata": {},
   "source": [
    "If no pre-built models exist for the use as solution for our problem, you can choose the option to build your own customized model from scratch. Upon collection of the required dataset for the training of the model, it is important that before you start, you have the necessary hardware that can support computationally intensive processes required for training. This often means that generic laptops and computers will not be able to handle the load necessary to train your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a41a9b",
   "metadata": {},
   "source": [
    "To solve this, you can choose the option of acquiring the GPU power of cloud services such as **Amazon Web Services** and **Google Colabs** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47431da5",
   "metadata": {},
   "source": [
    "For the development of your own customized model using Python using the Jupyter Notebook IDE. Necessary libraries that we can use for the development process are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97e284",
   "metadata": {},
   "source": [
    "**Pandas and NumPy libraries** for the access and modification of solid state data structures, n-dimensional matrices, and perform exploratory data analysis, and allows you to read CSV, JSON, and TSV data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd30376",
   "metadata": {},
   "source": [
    "**Matplotllib and Seaborn libraries** â€“ for the data visualization phase requiring the plotting of charts and graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89023dd0",
   "metadata": {},
   "source": [
    "**Scikit-learn, TensorFlow, MXNext, PyTorch,** and **Keras** framework libraries for the actual training of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
