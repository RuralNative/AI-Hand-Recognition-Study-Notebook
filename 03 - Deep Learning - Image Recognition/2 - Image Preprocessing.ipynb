{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0679874c-b9a9-4131-bbc3-846f4503c9ce",
   "metadata": {},
   "source": [
    "#  Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d0924-61d1-4c1a-a16e-75dbf122ca09",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be2892-8c4a-4056-918c-54f35a3e461c",
   "metadata": {},
   "source": [
    "Image preprocessing is the first step in the image recognition deep learning pipeline which is defined as a process involving the transformation of raw images into an appropriate form that fits your model, which usually includes adjustments to the size, orientation, and color of the images. The primary purpose of image preprocessing are the described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62724a8d-da13-4682-be1e-c2247b7986cb",
   "metadata": {},
   "source": [
    "**Improving Image Quality**: Preprocessing helps to raise the image's quality so that it can be analyzed more effectively by the model. It allows us to eliminate unwanted distortions and enhance specific qualities that are essential for the application we are working on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf12391-4c42-4cfc-b12f-1f8f4d108f83",
   "metadata": {},
   "source": [
    "**Standardizing Input**: The CNN's fully connected layers require that all images be in arrays of the same size. Preprocessing ensures that the input data's shape is consistent, which is necessary for the model to function correctly and produce the desired results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430d309-a147-45cc-a88a-af8e859b5aae",
   "metadata": {},
   "source": [
    "**Reducing Training Time**: It can shorten model training time and speed up model inference. For instance, if the input images are very large, reducing the size of these images can significantly decrease the amount of time needed to train the model without significantly affecting model performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5ae08-a3e8-487e-bced-329912c7e05e",
   "metadata": {},
   "source": [
    "**Reducing Complexity**: Preprocessing can be used to reduce the complexity of the applied algorithm. For example, converting color images to grayscale can reduce computational complexity, as color images contain more information than black and white images and can add unnecessary complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e068425-60d0-479b-ab16-b1ae90eca521",
   "metadata": {},
   "source": [
    "**Enhancing Algorithm Accuracy**: More often than not, preprocessing is used to conduct steps that increase the accuracy of the applied algorithm by reducing unwanted distortions or enhancing some image features crucial for subsequent processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df799197-06b8-44c7-b529-55ad2746496c",
   "metadata": {},
   "source": [
    "## Image Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05cbf5-3982-4745-b744-81c809634ecf",
   "metadata": {},
   "source": [
    "The image inputs for our convolutional neural network are interpreted 2-dimensional or 3-dimensional matrices/tensors by the computer, wherein each of the value in the matrice/tensor represent amplitude, or the intensity of the pixel. Different images are represented differently based on their identifying features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414745d9-94ff-4ff7-9909-2b822525dcd0",
   "metadata": {},
   "source": [
    "**Binary images** are images composed of only 2 colors, interpreted as a 2 - dimensional matrice/tensor, that have only two unique values of pixel intensity- 0 (representing black) and 1 (representing white). Binary images are useful for allowing an easy separation between the object and the background. It is often used for use cases where computational power is limited for both training and inference, and has the downside of losing a large amount of data that might prove necessary in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99211d97-d12a-461e-8a50-e0f46af05f82",
   "metadata": {},
   "source": [
    "**Grayscale (8-bit images)** are images composed of 256 unique colors, interpreted as a 2 - dimensional matrice/tensor, where a pixel intensity of 0 represents the black color and pixel intensity of 255 represents the white color. All the other 254 values in between are the different shades of gray. It offers a simplistic and efficient interpretation of colored images that are not as computationally intensive in terms of training and inference. They often perform better in identifying the illumination of objects, and prioritizes structural information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc2f173-d79b-4213-9fc8-51d04cb3be14",
   "metadata": {},
   "source": [
    "**RGB (Red Green Blue) images** are color images that are represented in the RGB color space and interpreted as 3 - dimensional matrices/tensors. Each pixel in an RGB image is represented as a combination of red, green, and blue intensities. The range of these intensities is usually from 0 to 255, where 0 represents no color (black) and 255 represents full intensity of the color. The combination of these three primary colors can produce a wide range of colors. For instance, when all three are at their full intensity (255,255,255), the resulting color is white. RGB images contain color information that can be useful for certain image recognition tasks and retains the most data for training which could be useful in certain cases, but has a downside of being highly computationally intensive and expensive for training and inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da62086-411c-4b18-a5b4-828f8210d29f",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799827f-2de2-46af-9f8b-523c4085434c",
   "metadata": {},
   "source": [
    "The following below are the different steps towards the complete preprocessing of data. Note that depending on the requirements of your neural network, a few of these might not be applied in your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b8ef5-be76-4e4e-8444-17fcad5161e1",
   "metadata": {},
   "source": [
    "**Image Dataset Creation**: Create a dataset of images from a directory containing labeled sub-directories of image inputs using appropriate TensorFlow utilities methods such as the [image_datasest_from_directory() function](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f48af5-0e0a-4ecf-bc45-680f558ba6be",
   "metadata": {},
   "source": [
    "**Resize Images**: Resize your images to a standard size. This is necessary because most deep learning frameworks require your training data to have the same shape. Standardized sizes ranged from 128x128 to 512x512 for best results. Common techniques for image resizing are *simple resizing*, *cropping*, and *padding*. \n",
    "* **Simple resizing** resizes the images using appropriate Python libraries and tools to fit the requirements of the neural network. \n",
    "* **Cropping** is a process of selecting the portion of the image and discarding the rest. This is a technique that has a significant downside as it results to the loss of data that may prove useful during training.\n",
    "* **Padding** is a process of adding pixels to an image, usually smaller than the required size of the neural network. This technique has a downside as it introduces noise and pixel alteration that may affect the training process negatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca41b47-b8f2-4c4a-b129-d325b7efcf3a",
   "metadata": {},
   "source": [
    "**Grayscale Image Conversion**: A preprocessing technique that aims to reduce the complexity of the input data by converting color images (RGB format) to grayscale images (single-channel format), thereby emphasizing the focus towards learning and understanding image's structural and textural information. It also reduces the input dimensionality to improve the computational efficiency and reduce the memory requirements of the model. \n",
    "* **\"Gleam\" method** is the most effective grayscaling technique and is the one I will use for my CNNs for image recognition, as recommended by this [research paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029740). You may opt for other methods depending on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d002f-81c6-45ec-a509-625d17634bc7",
   "metadata": {},
   "source": [
    "**Binary Image Conversion**: A preprocessing technique that aims to segment objects or regions of interest in an image through the application of a threshold to the grayscale image. Pixels are classified as either black or white based on their intensity values, simplifying the image and helping the model focus on specific areas or objects for classification. It also helps reduce noise or unwanted artifacts in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd270d1c-93a2-45a4-8559-87af72ad3098",
   "metadata": {},
   "source": [
    "**Image Normalization**: A preprocessing step that involves transforming the pixel values of an image to a standardized range or distribution. The main reasons for performing image normalization in the context of deep learning image recognition are listed below:\n",
    "* **Improved convergence**: Normalizing the pixel values of images can help improve the convergence of the training process. By scaling the pixel values to a common range, the optimization algorithm can more effectively update the model weights during training.\n",
    "* **Equalized feature importance**: Normalization ensures that all features (pixels) contribute equally to the learning process. If the pixel values have a large range or vary significantly, some features may dominate the learning process, leading to biased or suboptimal results.\n",
    "* **Reduced sensitivity to lighting conditions**: Normalization can help reduce the sensitivity of the model to variations in lighting conditions. By scaling the pixel values, the model becomes more robust to changes in brightness or contrast, making it more likely to generalize well to unseen data.\n",
    "* **Mitigation of numerical instability**: Deep learning models often involve computations that are sensitive to the scale of the input data. Normalization helps mitigate numerical instability issues, such as vanishing or exploding gradients, by keeping the values within a reasonable range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc78e0-4a93-4c20-bc40-f3a8ae45459e",
   "metadata": {},
   "source": [
    "**Data Augmentation**: A technique commonly used to increase the size and diversity of a training dataset by applying various transformations to the existing data. The goal of data augmentation is to improve the generalization and robustness of deep learning models by exposing them to a wider range of variations and patterns in the data. Note that jnot all data augmentation techniques should be applied in your dataset depending in the use case. In the context of image data, the following below describe the common data augmentation techniques:\n",
    "* **Horizontal and Vertical Flips**: Flipping an image horizontally or vertically to create new variations of the same image.\n",
    "* **Rotation**: Rotating an image by a certain angle to introduce variations in the orientation of objects.\n",
    "* **Translation**: Shifting an image horizontally or vertically to simulate different positions of objects within the image.\n",
    "* **Scaling**: Resizing an image to a different scale, either larger or smaller, to simulate variations in object sizes.\n",
    "* **Shearing**: Applying a shearing transformation to an image to introduce distortions and changes in perspective.\n",
    "* **Zooming**: Zooming in or out of an image to simulate variations in the distance or focus of objects.\n",
    "* **Brightness and Contrast Adjustments**: Modifying the brightness and contrast levels of an image to simulate different lighting conditions.\n",
    "* **Noise Injection**: Adding random noise to an image to make the model more robust to noisy or low-quality input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88d03a-dad0-4e07-9821-b5e6f59f689c",
   "metadata": {},
   "source": [
    "**Image Labeling**: A process of assigning one or more labels or categories to an image. It is done in the preprocessing step for an image recognition pipeline in the context of deep learning to prepare the data for training a deep learning model. The purpose of image labeling is to provide ground truth or annotations for the images, which can help the model learn to recognize and classify objects or patterns in the images more accurately. It is an essential part of data preparation for deep learning. It involves manually or automatically assigning labels or categories to the images in a dataset. These labels serve as the ground truth for training the deep learning model. It is also crucial for training supervised deep learning models. By assigning labels to the images, you provide the model with the correct information about the objects or patterns present in the images. This allows the model to learn the relationship between the input images and their corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b8c679-5123-4957-aa68-c903d4514dfd",
   "metadata": {},
   "source": [
    "**One-hot Encoding**: A technique used to represent categorical variables as binary vectors in machine learning and data analysis. It is commonly used when dealing with categorical data that cannot be directly used in mathematical models. E each category in a categorical variable is represented by a binary vector where only one element is \"hot\" (1) and the rest are \"cold\" (0). The length of the binary vector is equal to the number of unique categories in the variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
