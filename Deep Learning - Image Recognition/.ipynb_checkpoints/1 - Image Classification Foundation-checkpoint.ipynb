{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50337d1b-53ec-46bb-bb15-d1eb30b0aac3",
   "metadata": {},
   "source": [
    "# Image Classification - Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b6536-dcf0-4dfb-802b-5457da844a17",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2c8c2-6d81-44ee-81a1-4a7cb209a2b3",
   "metadata": {},
   "source": [
    "Image classification in deep learning refers to the process of training a model to classify images into different categories or classes. It is a fundamental task in computer vision and is widely used in various applications such as object recognition, face detection, and autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cc605-96fd-49da-ae60-7af8a7d1a0a7",
   "metadata": {},
   "source": [
    "Deep learning models for image classification are typically based on convolutional neural networks (CNNs). CNNs are designed to automatically learn and extract relevant features from images, making them well-suited for image classification tasks. The model consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad6e17-dd82-4c6a-9b7c-ba85b9421690",
   "metadata": {},
   "source": [
    "During the training phase, the model is fed with a large dataset of labeled images. It learns to recognize patterns and features in the images by adjusting the weights of the network through a process called backpropagation. The model is optimized to minimize the difference between its predicted outputs and the true labels of the training images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74eb3b6-1f47-48fe-8257-d0fb6b7904fb",
   "metadata": {},
   "source": [
    "Once the model is trained, it can be used to classify new, unseen images. The input image is passed through the network, and the model assigns a probability to each class. The class with the highest probability is considered as the predicted class for the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e8907-e7e5-42a8-b1e3-21e8407b7737",
   "metadata": {},
   "source": [
    "## Images as Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca1aba-9e49-4d4a-87d5-86ed4abdf43b",
   "metadata": {},
   "source": [
    "Images are collection of pixels, wherein each pixel contains information about the color or intensity of a specific point in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f4574-f1b3-415c-963a-d146723c77b8",
   "metadata": {},
   "source": [
    "For example, in a grayscale image, each pixel is represented by a single value that indicates the intensity of the pixel. The value can range from 0 (black) to 255 (white). This means that for a 640x480 grayscale image, it would contain 307,200 pixels or inputs. Images with higher resolutions would imply even larger input sizes for our neural network to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407471c-8a49-40f6-9290-16c61df760bc",
   "metadata": {},
   "source": [
    "In another example, a color image has each pixel is represented by three values (red, green, and blue) that determine the color of the pixel. These values can also range from 0 to 255. This means that for a 640x480 grayscale image, it would contain 307,200 pixels or inputs. Note that this only refers to one specific color, and a typical image are colored so this will be multiplied again by the three colors (RGB), providing a total of 921,600 inputs for our neural network to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c2685-815c-4110-963d-6097c495f258",
   "metadata": {},
   "source": [
    "With this being said, working with images are computationally intensive and it is highly recommended to work on smaller image sizes ranging between 128 pixels and 512 pixels wide, as anything larger than that would result in slower training periods. Due to this, images are scaled down in size before being fed into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6d4a5-fe91-457d-82f2-8b72cea5c486",
   "metadata": {},
   "source": [
    "## Image Classification Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b9c44-8220-4d52-b4f2-a910d70fc4a2",
   "metadata": {},
   "source": [
    "*A pipeline refers to a series of data transformation and modeling steps that are applied to a dataset to extract insights or make predictions., encompassing the entire sequential process from data collection and preprocessing to model training, evaluation, and prediction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9982df4-3c5e-4cda-a212-543f920eae10",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "A large dataset of labeled images is collected for training the deep learning model. Each image is associated with a specific class or label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fee439-70a5-4a78-ab4d-3ac54f8e8b68",
   "metadata": {},
   "source": [
    "**NOTES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65adf34-9903-43cf-81e7-c8622ba86231",
   "metadata": {},
   "source": [
    "Images of different categories must be uniform in size, otherwise **class imbalance** may occur. ***Class imbalance*** refers to a situation in which *the number of samples in each class of a dataset is not evenly distributed*, where there is a significant difference in the number of images available for different classes. This could result in a biased, non-performant, and inaccurate model that usually fails to perform well for the minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921eecf-9ca5-44c9-9874-7cf256d42e86",
   "metadata": {},
   "source": [
    "The best solution to class imbalance is preventing it in the first place by ensuring the number of pictures for each class is similar to that of other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03369e-58d0-4e96-9d84-7ec4eb289107",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The collected images are preprocessed to ensure consistency and improve the model's performance. This may involve resizing the images, normalizing pixel values, and applying data augmentation techniques to increase the diversity of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55de7f7-e2c0-4121-a78f-2c44a0585f1d",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "A process that involves dividing a dataset into separate subsets for training, validation, and testing purposes. The purpose of data splitting is to evaluate and validate the performance of a model on unseen data and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ab28a-9abd-4ced-ad6e-edf19452c6b4",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "A deep learning model, typically a convolutional neural network (CNN), is designed to learn and extract meaningful features from the input images. The model consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf21da2-fd71-4099-b3b8-14c660e22412",
   "metadata": {},
   "source": [
    "### Training\n",
    "The model is trained using the labeled images from the dataset. During training, the model learns to optimize its parameters by minimizing a loss function that measures the difference between the predicted labels and the true labels of the images. This is done through a process called backpropagation, where the gradients of the loss function with respect to the model's parameters are computed and used to update the parameters using an optimization algorithm, such as stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8839397-7bf5-4ce8-9071-bd72f3f3a658",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "After training, the model is evaluated on a separate validation set to assess its performance. Metrics such as accuracy, precision, recall, and F1 score are commonly used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f1a9b-5552-45a6-9a97-45930b40032c",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Once the model is trained and evaluated, it can be used to classify new, unseen images. The input image is fed into the trained model, which generates a probability distribution over the different classes. The class with the highest probability is considered as the predicted label for the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899cd49-83a2-45a3-a37b-ad772d48d33e",
   "metadata": {},
   "source": [
    "*It is important to note that the success of creating an image classification model relies on the availability of large and diverse dataset, and an effective and efficient model architecture.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727b3c7-e6b0-4367-92e7-57138d24179d",
   "metadata": {},
   "source": [
    "## Translational Invariance through CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b25abd-1dc2-4f19-a229-3b08efc8510a",
   "metadata": {},
   "source": [
    "Translational invariance refers to the ability of a system to recognize an object as the same object, regardless of where it is located in the image, which is achieved through the utilization of convolutional neural networks (CNN). This is crucial because it allows the system to maintain the identity and category of the object across changes in the visual input, such as the relative positions of the viewer/camera and the object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aaf232-d3d5-4ef4-b421-4b6d346d2d24",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8e435-5734-498c-a416-82e45cf49bf6",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNN) are the backbone of deep learning-based image classification. They are designed to automatically learn hierarchical representations of images by applying convolutional filters and pooling operations. CNNs excel at capturing local patterns and spatial relationships in images, making them well-suited for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68bfcd-d7d5-4ae1-bc4c-22892a235fcf",
   "metadata": {},
   "source": [
    "*Traditional artificial neural networks are not used as it is disadvantageous to use in the context of image classification deep learning tasks, enumerated below:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d65c86-0bc6-441f-916e-19dacae5dd16",
   "metadata": {},
   "source": [
    "**Loss of Spatial Information**: Traditional ANNs treat each input feature independently and do not consider the spatial relationships between pixels in an image. This results in the loss of important spatial information, which is crucial for understanding the structure and context of an image. Without considering spatial information, the performance of traditional ANNs in image classification tasks may be significantly compromised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ea827-62be-4837-b9a4-8b86b0f2b728",
   "metadata": {},
   "source": [
    "**High Dimensionality**: Images are high-dimensional data, typically represented as a grid of pixels. Traditional ANNs require a fixed-size input vector, which means that images need to be flattened into a 1D vector before being fed into the network. This flattening process results in a loss of the inherent structure and organization of the image, making it challenging for the network to effectively learn and represent complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056d26b-260b-4e0c-a4d3-2221ae3f5cd8",
   "metadata": {},
   "source": [
    "**Limited Local Feature Extraction**: Traditional ANNs lack the ability to effectively extract local features from images. They do not have built-in mechanisms to capture spatial hierarchies or detect local patterns. This can lead to difficulties in recognizing and distinguishing important features in images, especially when dealing with complex and varied visual content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0e726-7340-43fd-898a-74a3647b8162",
   "metadata": {},
   "source": [
    "**Large Number of Parameters**: Traditional ANNs require a large number of parameters to learn and represent the relationships between input features. When applied to image classification, this can result in a massive number of parameters due to the high dimensionality of images. Training such networks becomes computationally expensive and requires a large amount of labeled data to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbcaa61-41ea-4e8b-aaaf-25eea0a8fb8f",
   "metadata": {},
   "source": [
    "**Limited Robustness to Variations**: Images can exhibit variations in lighting conditions, scale, rotation, and other transformations. Traditional ANNs may struggle to handle these variations effectively, as they lack the built-in mechanisms to capture and generalize from such transformations. CNNs, on the other hand, are designed to be more robust to these variations through their local receptive fields and pooling operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b5998-4d52-4b9b-baf4-695bf8f8e311",
   "metadata": {},
   "source": [
    "*CNNs are specifically designed to address the challenges and characteristics of image data, allowing them to capture spatial information, extract local features, and handle variations more effectively. CNNs have demonstrated superior performance in image classification tasks and have become the standard approach in the field*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
