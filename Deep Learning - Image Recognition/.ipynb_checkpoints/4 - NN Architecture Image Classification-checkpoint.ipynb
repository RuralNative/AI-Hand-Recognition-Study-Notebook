{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90be4ef4-1593-4520-aaa2-e5e7b17fa887",
   "metadata": {},
   "source": [
    "# Neural Network Image Classification Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6c8b0-345f-4743-b2c8-1a48e5425ff4",
   "metadata": {},
   "source": [
    "*The information described below details the structure of the neural network specifically made for the purpose of image classification:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b3f75-d029-419d-abca-c9e851f51184",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f05dd9-bdad-47f1-9e07-b8445f2cf85f",
   "metadata": {},
   "source": [
    "The input layer is responsible for receiving the raw image as input of the neural network. It ensures that the image data is in a suitable format for efficient and effective processing in the subsequent layers of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ac011-eff2-44c9-a509-76eed2d089f4",
   "metadata": {},
   "source": [
    "In the input layer, the image is represented as a grid of pixels with individual pixels containing numerical values ranging from 0 to 255. These values represent the intensity of the color; with a vector of 3 numbers for 3 - channel images (RGB) and a single value for grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc3715-10d6-47de-bec7-b0e5ff41426d",
   "metadata": {},
   "source": [
    "The input layer may receive either a 2-dimensional tensor or a 3-dimensional tensor depending on the nature of input data it receives. For cases where we use a colored image, it receives a 3 - dimensional tensor of height, width, and 3-channel values. On the other hand if it recieves a grayscale image, then it only requires a 2 - dimensional tensor for height and width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22e9f2-91b7-4160-a535-a83f88ea3df9",
   "metadata": {},
   "source": [
    "Note that before our images are fed to the neural network for training, they are preprocessed to a suitable image format. This includes resizing them to a consistent size, normalizing the pixel values, and may also include data augmentation such as rotation to increase the diversity of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fe309-6754-4a6e-9389-067e3216c6f4",
   "metadata": {},
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b5402-b246-49b5-8033-ed857cebc9a1",
   "metadata": {},
   "source": [
    "A **convolutional layer** performs a ***convolution operation*** to *detect specific features in the input data, producing one or more feature maps*. It uses filters, stride, padding, and an activation function to perform this operation, and it shares parameters to increase efficiency and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1278d00-9719-4edb-b48f-fccda27dfa89",
   "metadata": {},
   "source": [
    "**Filters or kernels** are *small matrices of weights that the network learns during training*. ***Each filter is responsible for detecting a specific feature in the input data***, such as edges, corners, or more complex patterns in higher layers. The number of filters in a convolutional layer determines the number of feature maps it will produce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50262d46-39f1-48bd-9e06-55a0561d3ae3",
   "metadata": {},
   "source": [
    "The **convolutional operation** refers to an *operation involving sliding the filter (kernel) across the input data (image) and computing the dot product between the filter and the input at each position*. This results to a feature map that represents the presence of specific features detected by the filter in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23bd250-09af-40d0-aca4-2b56e2f6f7fb",
   "metadata": {},
   "source": [
    "*How our kernel is slide through the image during the convolutional operation is determined by the value of stride*. The **stride** is the ***number of positions the filter moves at each step during the convolution operation***. A stride of 1 means the filter slides one position at a time, while a stride of 2 means it jumps two positions at each step. The ***stride affects the size of the output feature map***: a *larger stride results in a smaller feature map*, while *smaller side results in a larger feature map*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36c324-4a9b-45d4-b587-1390e8ec2c2d",
   "metadata": {},
   "source": [
    "In situations where a kernel is slid beyond the edge of our image data, resulting in certain filter points scanning non-existent pixel data, padding is involved to determine how our kernels approach such situation. **Padding** involves ***adding extra pixels around the border of the input data***. This is done *to control the spatial size of the output feature maps and to preserve the spatial dimensions of the input*. **'Valid' padding*** means ***no padding is used (the filter does not go outside the input)***, and **'same' padding** means ***enough padding is added to keep the output size the same as the input size.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c2d64-0c3c-4e89-9b12-9429c6498a0e",
   "metadata": {},
   "source": [
    "An **activation function**, usually in the form of a ***rectified linear unit (ReLU)***, is introduced after the convolutional operation to *introduce non - linearity into the model*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484f7b5-9eb0-407f-9c37-8b03406c1cd2",
   "metadata": {},
   "source": [
    "One of the **benefits of adding a convolutional layer to our neural network** is due to the fact that ***the same filter is applied to different parts of the input***, meaning *the same weights are used multiple times*. This ***parameter sharing reduces the number of parameters in the model***, making it *more efficient* and *less prone to overfitting* compared to fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb868e46-7d92-47a2-a1ac-a06e8fce60ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df82bbcd-b1c0-42e8-984b-f014a9b94eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
