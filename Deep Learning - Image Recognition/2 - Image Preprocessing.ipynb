{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0679874c-b9a9-4131-bbc3-846f4503c9ce",
   "metadata": {},
   "source": [
    "#  Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d0924-61d1-4c1a-a16e-75dbf122ca09",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be2892-8c4a-4056-918c-54f35a3e461c",
   "metadata": {},
   "source": [
    "Image preprocessing is the first step in the image recognition deep learning pipeline which is defined as a process involving the transformation of raw images into an appropriate form that fits your model, which usually includes adjustments to the size, orientation, and color of the images. The primary purpose of image preprocessing are the described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62724a8d-da13-4682-be1e-c2247b7986cb",
   "metadata": {},
   "source": [
    "**Improving Image Quality**: Preprocessing helps to raise the image's quality so that it can be analyzed more effectively by the model. It allows us to eliminate unwanted distortions and enhance specific qualities that are essential for the application we are working on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf12391-4c42-4cfc-b12f-1f8f4d108f83",
   "metadata": {},
   "source": [
    "**Standardizing Input**: The CNN's fully connected layers require that all images be in arrays of the same size. Preprocessing ensures that the input data's shape is consistent, which is necessary for the model to function correctly and produce the desired results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430d309-a147-45cc-a88a-af8e859b5aae",
   "metadata": {},
   "source": [
    "**Reducing Training Time**: It can shorten model training time and speed up model inference. For instance, if the input images are very large, reducing the size of these images can significantly decrease the amount of time needed to train the model without significantly affecting model performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5ae08-a3e8-487e-bced-329912c7e05e",
   "metadata": {},
   "source": [
    "**Reducing Complexity**: Preprocessing can be used to reduce the complexity of the applied algorithm. For example, converting color images to grayscale can reduce computational complexity, as color images contain more information than black and white images and can add unnecessary complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e068425-60d0-479b-ab16-b1ae90eca521",
   "metadata": {},
   "source": [
    "**Enhancing Algorithm Accuracy**: More often than not, preprocessing is used to conduct steps that increase the accuracy of the applied algorithm by reducing unwanted distortions or enhancing some image features crucial for subsequent processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df799197-06b8-44c7-b529-55ad2746496c",
   "metadata": {},
   "source": [
    "## Image Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05cbf5-3982-4745-b744-81c809634ecf",
   "metadata": {},
   "source": [
    "The image inputs for our convolutional neural network are interpreted 2-dimensional or 3-dimensional matrices/tensors by the computer, wherein each of the value in the matrice/tensor represent amplitude, or the intensity of the pixel. Different images are represented differently based on their identifying features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414745d9-94ff-4ff7-9909-2b822525dcd0",
   "metadata": {},
   "source": [
    "**Binary images** are images composed of only 2 colors, interpreted as a 2 - dimensional matrice/tensor, that have only two unique values of pixel intensity- 0 (representing black) and 1 (representing white). Binary images are useful for allowing an easy separation between the object and the background. It is often used for use cases where computational power is limited for both training and inference, and has the downside of losing a large amount of data that might prove necessary in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99211d97-d12a-461e-8a50-e0f46af05f82",
   "metadata": {},
   "source": [
    "**Grayscale (8-bit images)** are images composed of 256 unique colors, interpreted as a 2 - dimensional matrice/tensor, where a pixel intensity of 0 represents the black color and pixel intensity of 255 represents the white color. All the other 254 values in between are the different shades of gray. It offers a simplistic and efficient interpretation of colored images that are not as computationally intensive in terms of training and inference. They often perform better in identifying the illumination of objects, and prioritizes structural information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc2f173-d79b-4213-9fc8-51d04cb3be14",
   "metadata": {},
   "source": [
    "**RGB (Red Green Blue) images** are color images that are represented in the RGB color space and interpreted as 3 - dimensional matrices/tensors. Each pixel in an RGB image is represented as a combination of red, green, and blue intensities. The range of these intensities is usually from 0 to 255, where 0 represents no color (black) and 255 represents full intensity of the color. The combination of these three primary colors can produce a wide range of colors. For instance, when all three are at their full intensity (255,255,255), the resulting color is white. RGB images contain color information that can be useful for certain image recognition tasks and retains the most data for training which could be useful in certain cases, but has a downside of being highly computationally intensive and expensive for training and inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da62086-411c-4b18-a5b4-828f8210d29f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6799827f-2de2-46af-9f8b-523c4085434c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d80b8ef5-be76-4e4e-8444-17fcad5161e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "191c5e11-d04c-4ced-b03a-28893e4f2516",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b9d002f-81c6-45ec-a509-625d17634bc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd270d1c-93a2-45a4-8559-87af72ad3098",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ccc78e0-4a93-4c20-bc40-f3a8ae45459e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b88d03a-dad0-4e07-9821-b5e6f59f689c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1b8c679-5123-4957-aa68-c903d4514dfd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96f5161c-5457-4eef-99d4-adefb5284c61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74f1194a-cc1f-47bc-8300-a6e79a071680",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3127e89c-1e16-40f5-8e1f-8360ca9322b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aee5d3f4-f96e-4d4c-96e7-2032aa6ccaf4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16aa81-55dc-4f97-9588-2a5c87708d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b659b04d-4144-42ac-8ef1-d1a651bafefd",
   "metadata": {},
   "source": [
    "## Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf6748-6554-41a4-80c6-8baa3e6fac95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "959d4e6a-a95a-4145-a005-197cc7d59a3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "865be856-7771-48f4-8eb0-7b0187d8fcf0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af741e90-937e-472d-a673-26cc507cc365",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0f276dd-d8f8-4733-a5bd-3b1d300ccf15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b8a1435-c87b-44d4-97c2-071465669634",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dddd9608-9022-4fb8-b94c-5c0384076104",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "972906d8-fbd7-4d5a-b003-fb986783fda7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f83fd2c1-d308-4ca2-a960-88017762a35e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501a4b2-dd7a-44f2-a870-42e9a460aeb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe698b72-546f-46d0-9750-5f787503d94a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
