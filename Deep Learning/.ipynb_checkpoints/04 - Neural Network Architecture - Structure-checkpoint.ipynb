{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec3594a-344c-41d8-808e-6932a13e8146",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f2bf3-cc97-4410-bcad-94a37aab99b6",
   "metadata": {},
   "source": [
    "A **neural network architecture** refers to the *structure or layout of a neural network*. It defines how the individual nodes or layers are organized and connected to each other. The architecture *determines how the network processes and transforms input data to produce output predictions or classifications*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c3ab5-dbc0-4c6b-b3ef-9deb3f1a24c1",
   "metadata": {},
   "source": [
    "**Direction** definition is *PENDING*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a5e1a-b7c2-4ad2-acea-51927cdbb324",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49bbbb-e547-4b5e-a2ea-87a749081764",
   "metadata": {},
   "source": [
    "The **input layer** is responsible in *preprocessing and transforming the input data to its appropriate numeric representations prior* to being passed to every node in the first hidden layer. It ***does not have its own weights, biases, or activation functions***; as they only come to play inside the hidden layers. The input layer ensures that the neural network can *better handle different types of data*, *improve convergence during training*, and *enhance the overall performance of the model*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeb83b8-d0f9-4d22-a05c-6d444c9bd0b1",
   "metadata": {},
   "source": [
    "*Below are the specific and detailed steps performed by the input layer for the data preprocessing:*\n",
    "* **Data Scaling/Normalization**: This involves scaling the input data to a specific range or normalizing it to have zero mean and unit variance. Scaling the data ensures that all features have a similar scale, which can help improve the performance and convergence of the neural network during training.\n",
    "* **Data Encoding**: If the input data contains categorical variables, they may need to be encoded into numerical values. This can be done using techniques such as one-hot encoding, label encoding, or ordinal encoding, depending on the nature of the categorical variables.\n",
    "* **Missing Data Handling**: If the input data contains missing values, preprocessing in the input layer may involve handling these missing values. This can be done by imputing the missing values with a suitable strategy, such as mean imputation, median imputation, or using more advanced techniques like regression imputation or multiple imputation.\n",
    "* **Feature Selection/Extraction**: In some cases, the input data may contain a large number of features, and not all of them may be relevant for the task at hand. Preprocessing in the input layer may involve selecting a subset of the most informative features or performing feature extraction techniques, such as principal component analysis (PCA) or linear discriminant analysis (LDA), to reduce the dimensionality of the input data.\n",
    "* **Data Reshaping**: Depending on the specific requirements of the neural network architecture, the input data may need to be reshaped or restructured. For example, if the network expects a certain input shape, such as a specific number of rows and columns, the input data may need to be reshaped accordingly.gly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d9d78-6579-4b87-a162-6b592818b043",
   "metadata": {},
   "source": [
    "## Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69f31c-8dba-4d4b-bd75-b2ba66517b73",
   "metadata": {},
   "source": [
    "The **hidden layers** serves as the brain of the neural network, responsible in ***processing the data to capture and determine the complex relationships and relationship existent in them*** leading to *improved generalization and prediction capabilities in the output layer*. Each node in the individual hidden layers processes and identifies relationships between the target variable (y) and the feature variables (x), and passes their output to all nodes found in the succeeding layer, persisted with their passed weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121a15f-d0e3-4333-bf6c-0458df9f958b",
   "metadata": {},
   "source": [
    "*Below are the specific and detailed tasks performed by the hidden layer:*\n",
    "* **Weight and Bias Adjustment**: The hidden layers contribute to determining the optimal values of weights and biases. The weights control the strength of the connections between neurons, while the biases introduce an additional parameter that helps in shifting the activation function. By adjusting these parameters, the hidden layers allow the neural network to learn and adapt to the patterns in the data.\n",
    "* **Non-Linear Transformations**: The primary function of the hidden layers is to perform non-linear transformations on the inputs. Each neuron in the hidden layer applies an activation function to the weighted sum of its inputs. This introduces non-linearity into the model, enabling the neural network to capture complex relationships and patterns in the data. The hidden layers allow the network to learn and represent non-linear mappings between the input and output variables.\n",
    "* **Feature Extraction and Representation**: The hidden layers also play a crucial role in feature extraction and representation learning. As the data passes through the hidden layers, they learn to extract relevant features from the input data. Each hidden layer can learn to detect and represent different levels of abstraction and complexity. By combining these learned features, the network can make more accurate predictions in the output layer.\n",
    "* **Hierarchical Learning**: Neural networks with multiple hidden layers can learn hierarchical representations of the data. The early hidden layers learn low-level features, such as edges or corners, while the deeper hidden layers learn more abstract and high-level features. This hierarchical learning allows the network to capture intricate patterns and relationships in the data, leading to improved performance.\n",
    "* **Generalization and Prediction**: The hidden layers, along with the learned weights and biases, contribute to the network's ability to generalize and make predictions. By learning from the training data, the hidden layers adjust the weights and biases to minimize the difference between the predicted output and the actual output. This enables the network to make accurate predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bf04d-5ff7-4711-b41b-d702d77e9fd7",
   "metadata": {},
   "source": [
    "Usually more hidden layers means better accuracy, but is not always true in some occassions. It is also important to consider the fact that more hidden layers meant higher processing power and time needed for training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f97ac-58a8-4e6c-99c4-2134b4abce5c",
   "metadata": {},
   "source": [
    "## Weights and Biases in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968592de-2589-4341-aab9-42a9d0a4c840",
   "metadata": {},
   "source": [
    "***Weights** and **biases** are fundamental components of a neural network and are its trainable parameters determined during the training process to improve the model's prediction capabilities. They are in a form of arrays storing numeric values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efebfc1-75db-4883-82e5-01cd995e24bb",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95a622-9512-46b5-a792-f0c8a863397c",
   "metadata": {},
   "source": [
    "**Weights** are parameters associated with the connections between neurons in a neural network. Each connection between two neurons has a weight assigned to it. These weights determine the strength or importance of the connection. They ***determine the contribution of each input to the output of a node***. *Larger weights* amplify the input's effect, while *smaller weights* diminish it. By adjusting the weights, the network learns to assign appropriate importance to different features and patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d28dd-b2bc-4c6f-91d9-05fea7f959b8",
   "metadata": {},
   "source": [
    "Initially, weights are assigned randomly using **initialization techniques** such as *random initialization*, *Xavier initialization*, and *He initialization*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97cbba-2358-4cea-af02-fb2476556d9f",
   "metadata": {},
   "source": [
    "During training, the network updates the weights using **optimization algorithms** like *gradient descent*. The goal is to minimize the difference between the *predicted output* and the *actual output* through such algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05842591-f94b-44d3-93f2-a2f937bdad0e",
   "metadata": {},
   "source": [
    "### Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29610f2-83a8-4b9b-ba63-6e6b57daa3b0",
   "metadata": {},
   "source": [
    "**Biases** are additional parameters in neural networks that allow for shifting the activation function. Each neuron, with the exception of the input layer nodes, has a bias associated with it. Biases help the network learn and represent non-linear relationships between inputs and outputs. They allow the network to ***introduce a certain level of flexibility and adaptability***. They help in capturing non-linear patterns and making predictions that are not solely dependent on the input values. Biases shift the activation function, enabling the network to learn complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2605c1f-f459-4165-88e7-f0db66cec721",
   "metadata": {},
   "source": [
    "Similar to weights, biases are initialized with random values. The initialization process depends on the specific activation function used in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ee4d7-e78b-40a3-840f-60f7928943d8",
   "metadata": {},
   "source": [
    "During training, biases are updated along with the weights. The network learns the optimal bias values that help in achieving the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc19fe-54be-401b-80fd-3991b0e9b366",
   "metadata": {},
   "source": [
    "## Activation Functions in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed84cc8-43d8-45c4-b73d-d0418de7a05d",
   "metadata": {},
   "source": [
    "**Activation functions** play a crucial role in deep learning by introducing ***non-linearity*** into the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03233a9-aef5-43e4-9d40-3c464c5f33ad",
   "metadata": {},
   "source": [
    "*Activation functions* are applied to the weighted sum of inputs in a neuron to introduce non-linearity. Without activation functions, a neural network would be limited to learning linear relationships between inputs and outputs. Activation functions allow the network to learn and represent complex patterns and non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d460fc-f376-47b5-abf6-c13dce1f7691",
   "metadata": {},
   "source": [
    "### Non - Linearity through Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16370359-095d-4055-97df-dc5742dc5fe4",
   "metadata": {},
   "source": [
    "**Non-linearity** refers to the property of a function or relationship where the output does not change linearly with the input. ***Non-linear transformations***, introduced through activation functions in deep learning, enable neural networks to capture and model complex patterns and non-linear relationships in the data. This is usually achieved by transforming the input values into a desired output range, typically between 0 and 1 or -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b27658-9b76-47e6-972d-029a9c2ee98e",
   "metadata": {},
   "source": [
    "In the context of deep learning, non-linearity is crucial because many real-world problems and data patterns are inherently non-linear. Linear relationships can only capture simple patterns and relationships that can be represented by straight lines or planes. However, complex patterns and relationships often require non-linear transformations to be accurately captured and modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85608d-7f7a-4a9d-86e9-ea7614b5a58d",
   "metadata": {},
   "source": [
    "By introducing non-linearity through activation functions, neural networks can learn and represent these non-linear relationships. Activation functions allow the network to capture intricate patterns, make complex decisions, and approximate highly non-linear functions. This enables deep learning models to handle a wide range of tasks, such as image recognition, natural language processing, and time series prediction, where non-linear relationships are prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e3045-e500-45b7-99bc-1a36535f21be",
   "metadata": {},
   "source": [
    "### Common Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3ab65-d498-471d-9457-99979d94cce4",
   "metadata": {},
   "source": [
    "*The choice of activation function depends on the problem at hand and the characteristics of the data. Different activation functions have different properties and can impact the network's performance. It is important to experiment with different activation functions and select the one that yields the best results for a specific task.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99feb53a-6143-499d-8943-8c7f99720ab0",
   "metadata": {},
   "source": [
    "**Sigmoid Function**: The sigmoid function is a popular activation function that ***maps the input to a value between 0 and 1***. It has a smooth S-shaped curve. The sigmoid function is useful in binary classification problems or when the output needs to be interpreted as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8b8a1-59db-43ff-bd80-5dd0116a6a2f",
   "metadata": {},
   "source": [
    "**ReLU (Rectified Linear Unit)**: The ReLU activation function is widely used in deep learning. It ***returns the input if it is positive, and 0 otherwise***. ReLU helps in overcoming the vanishing gradient problem and accelerates the convergence of the network during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9cf7c6-9b77-45a2-8f1f-ec40c31b01d6",
   "metadata": {},
   "source": [
    "**Leaky ReLU**: Leaky ReLU is a variation of the ReLU function that ***introduces a small slope for negative inputs***. This helps in addressing the \"dying ReLU\" problem, where neurons can become inactive and stop learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85651ce4-be06-4311-aa65-4fd1e970ace7",
   "metadata": {},
   "source": [
    "**Tanh (Hyperbolic Tangent)**: The tanh function ***maps the input to a value between -1 and 1***. It is similar to the sigmoid function but centered around 0. Tanh is useful when the input data is normalized or when negative values are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad644f0f-9617-40c9-a969-430e45c6b92e",
   "metadata": {},
   "source": [
    "**Softmax**: The softmax function is commonly used in the output layer of a neural network for multi-class classification problems. It ***converts the output values into probabilities, where the sum of all probabilities is equal to 1***. Softmax is useful when dealing with mutually exclusive classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571fea12-bc08-4c01-ba9c-dbf0053bc146",
   "metadata": {},
   "source": [
    "*Note that each hidden layer may have a different activation function from other hidden layers of the same neural network to improve the performance of the neural network.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bb856-2a3e-411b-9682-73addc258287",
   "metadata": {},
   "source": [
    "## Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec38a0c-3b88-4921-84e3-8d75978d7662",
   "metadata": {},
   "source": [
    "The **output layer** in a deep neural network is the final layer of neurons that ***produces the network's predictions or outputs***. It is responsible for *mapping the learned features and representations* from the *preceding layers to the desired output format*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8a07e-1d76-44b3-b5e9-d02b1949ece7",
   "metadata": {},
   "source": [
    "The **number of neurons** in the output layer ***depends on the specific task and the desired output format***. For example, in *binary classification*, there is typically *one node in the output layer* to represent the probability or prediction for one class. In *multi-class classification*, the *number of neurons in the output layer corresponds to the number of classes*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520993d-9859-470a-a9b3-cbb55ae5e5b2",
   "metadata": {},
   "source": [
    "The choice of activation function in the output layer depends on the nature of the problem and the desired output format. Common activation functions used in the output layer with relation to the nature of the problem are listed below:\n",
    "* **Sigmoid Function**: Used for ***binary classification problems***, the sigmoid function ***maps the output to a value between 0 and 1***, representing the probability of belonging to a particular class.\n",
    "* **Softmax Function**: Used for ***multi-class classification problems***, the softmax function ***normalizes the outputs across all classes, producing a probability distribution where the sum of probabilities is equal to 1***. It helps in determining the most likely class for a given input.\n",
    "* **Linear Function**: Used for ***regression problems***, the linear activation function allows the network to ***directly output continuous values without any transformation***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f845260-8d29-4892-8ea4-db876845211d",
   "metadata": {},
   "source": [
    "*It is also important to note that in the output layer we also apply the loss functions*. The **loss function** measures the ***discrepancy between the predicted output and the true output***. Common loss functions include **mean squared error (MSE)** for *regression problems*, **binary cross-entropy** for *binary classification*, and **categorical cross-entropy** for *multi-class classification*. We will discuss this later to expand on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a574d5-c446-4e22-b33c-63a3796ef9e9",
   "metadata": {},
   "source": [
    "The outputs derived after being processed in the output later are then run through an **interpretation process**. For example, in *binary classification*, the output can be ***interpreted as the probability of belonging to a particular class***. In *multi-class classification*, the output can be ***interpreted as the probabilities of belonging to each class***. In *regression*, the ***output represents the predicted continuous value***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54f1f7-cb48-491a-b999-d0c24df82751",
   "metadata": {},
   "source": [
    "## Loss Function in Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054610f-9c16-40db-aeaf-d1513c697b62",
   "metadata": {},
   "source": [
    "A **loss function** in the output layer of a neural network is to ***measure the discrepancy or error between the predicted output and the true output (also known as the ground truth)***. The loss function quantifies how well the network is performing on the given task and provides a measure of the network's ability to make accurate predictions. Below are the major aspects of a loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949df433-f125-4dd9-8e82-22475ba40d3f",
   "metadata": {},
   "source": [
    "The loss function is used as an **error measurement** calculates the difference between the predicted output and the true output. It provides a numerical value that represents the error or discrepancy between the predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f488f3-18b0-4a9a-b960-07d7e3044146",
   "metadata": {},
   "source": [
    "The loss function is used as an **optimization guide** for the network's parameters (weights and biases) during the training process. The goal is to minimize the loss function by adjusting the parameters, which leads to better predictions and improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2d5a8-b2ad-46a7-bf64-2861e0f7080a",
   "metadata": {},
   "source": [
    "The loss function is **task-specific**. The choice of the loss function depends on the specific task and the desired output format. Different tasks, such as regression, binary classification, or multi-class classification, require different loss functions.\n",
    "* **Mean Squared Error (MSE)**: Used for regression tasks, MSE calculates the average squared difference between the predicted and true values. It penalizes larger errors more heavily.\n",
    "* **Binary Cross-Entropy**: Used for binary classification tasks, binary cross-entropy measures the dissimilarity between the predicted probabilities and the true binary labels. It is commonly used with sigmoid activation in the output layer.\n",
    "* **Categorical Cross-Entropy**: Used for multi-class classification tasks, categorical cross-entropy measures the dissimilarity between the predicted class probabilities and the true class labels. It is commonly used with softmax activation in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8fc8f-d598-40f7-b03e-9779b165d33c",
   "metadata": {},
   "source": [
    "The loss function is used for **training and evaluation**. During training, the loss function is used to compute the gradient of the loss with respect to the network's parameters. This gradient is then used to update the parameters through backpropagation and gradient descent optimization. The loss function guides the learning process by indicating the direction in which the parameters should be adjusted to minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df8c90-d260-4de2-a7a8-b944d3f9647e",
   "metadata": {},
   "source": [
    "Lastly, the loss function is often used as an **evaluation metric** to assess the performance of the trained model. However, it may not always directly reflect the model's overall performance. Additional evaluation metrics, such as accuracy, precision, recall, or F1 score, may be used to provide a more comprehensive assessment of the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
