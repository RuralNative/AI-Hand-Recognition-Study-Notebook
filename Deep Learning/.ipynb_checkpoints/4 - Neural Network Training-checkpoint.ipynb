{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abdc044-f227-40d0-aad6-d97cfc215936",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1112a8-fc76-46d4-a326-8700aa637804",
   "metadata": {},
   "source": [
    "*In this notebook, we shall discuss the following steps to perform in order to train a neural network*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6dbc3-49a2-4f18-93ba-be8fbf8d202c",
   "metadata": {},
   "source": [
    "## Setting Up for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995143ef-5b0b-4f4b-8722-da9c70970ff6",
   "metadata": {},
   "source": [
    "*Before we begin the training process, it is important to set up our data which is to be fed for training. Similar to machine learning, we must preprocess and split our data to ensure an efficient model performance. These steps will be discussed in depth in this section.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e232d-3735-4e36-949f-43c392d04409",
   "metadata": {},
   "source": [
    "### Splitting Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436f67f-6d96-4378-bc56-17b70c55f49f",
   "metadata": {},
   "source": [
    "Our input data should be split into appropriate sizes for different roles that serves unique roles towards creating an efficient and effective model. The data will be split into *three (3) sets*: (1) **training data set**, (2) **validation data set**, and (3) **test data set**. The input data is split and collected to this sets following an ***80:10:10*** ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f983c87-7e51-4ae8-8d20-7a60986e09a6",
   "metadata": {},
   "source": [
    "### Define the ANN Architecture and Hyper Parameter Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097d82b-125b-4fec-aac9-23aebfea177a",
   "metadata": {},
   "source": [
    "After setting up our data set in preparation of being fed to our neural network, we must now decide on the initial values for our neural networks parameters and hyper parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ef7cf-a92e-4fa0-8b44-550557d5486e",
   "metadata": {},
   "source": [
    "***First***, we have to **decide in the architecture of our artificial neural network**. This is done by defining the initial number of layers and their respective nodes for our neural network. Note that there is always **ONLY ONE (1)** input and output layers, and it is only the number of hidden layers we are trying to initially define. We then provide the activation functions for each layers, with the exception of the input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951ec77-7498-4acb-bf95-35a3fd81c244",
   "metadata": {},
   "source": [
    "***Second***, we have to ***define our hyper parameters*** for our neural network. This includes the *epoch*, *batch sizes*, and *error functions*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5aae8-2ceb-434c-834b-a3c3292bd7a1",
   "metadata": {},
   "source": [
    "Note that our intial values for our parameters and hyperparameters will often need adjustments as we train our model and analyze its results. Adjust the values if needed and follow best practices if available for given problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c25d808-fa54-46ac-bf85-54281859cf50",
   "metadata": {},
   "source": [
    "### Weights/Biases Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96d462-a993-4ad5-ac55-a8d28d3135c1",
   "metadata": {},
   "source": [
    "With our neural network's parameters and hyperparameters initialized, we can now move on to the last step included in our setup process, and that is the initialization of our weights and biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19531cd5-6b15-4a9c-bf7f-9e75af254fb3",
   "metadata": {},
   "source": [
    "This is achieved using multiple initialization techniques. One of them is **zero initialization**, something that is not recommended due to it assigning all weights and biases to zero (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565e4ef-91e6-4847-9773-fa40b1020e79",
   "metadata": {},
   "source": [
    "Another one is **random initialization**, which initializes random values from a standard normal distribution (mean = 0, SD = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed196f-674f-4e3d-96f9-a213eb18623d",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8d6cd-b451-4435-b59f-bfa22504db4c",
   "metadata": {},
   "source": [
    "*Upon the completion of our setup we can then begin with our training process. This section shall discuss the training process of the neural network supplemented by the definitions of the important terminologies or steps used during this process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85d8ad-5ab0-4e3c-a83e-80682a6dc04b",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86527431-08ae-479f-91dd-f6c300781a4c",
   "metadata": {},
   "source": [
    "Our neural network will first perform one round of forward propagation. **Forward propagation** is the process of running the neural network training with its defined parameters and hyperparameters to make predictions of the ***supposed values of the sample's target variables (y-hat)*** compared to its ***actual target variable values provided by our training data set (y)***. The predicted and actual values would then be compared to determine their error rates and adjust the values of the neural network's parameters and hyperparameters based from this on the subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4375c-48bd-4bce-9a9f-35ce6b89a035",
   "metadata": {},
   "source": [
    "For example, we may have an training data set with 10 samples for the first round of forward propagation. This 10 samples has their corresponding target variable (y) values defined as (1,2,3,4,5,6,7,8,9,10). During the forward propagation, the neural network runs itself with adherence to the set of values of its parameters and hyperparameters and predicted that the values of the samples' target variable (y) as (2,3,4,4,5,6,8,10,19,21). Once the forward propagation is done, the predicted and actual values would then be compared to determine their error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db33cc0-7c09-437d-a738-0381c1a32a4e",
   "metadata": {},
   "source": [
    "*The purpose of forward propagation is to determine the effectiveness of our neural network with its set of parameters and hyperparameters to predict the actual values of our training data set's target variables. The outcome of this step would then be the basis of how our neural network will be further modified to create an accurate and effective model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6f567-1f6d-48d3-bc18-9a3a5df16250",
   "metadata": {},
   "source": [
    "### Measuring Accuracy and Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88eb1a-f361-4115-9135-83cb20a79547",
   "metadata": {},
   "source": [
    "Using the output of our forward propagation round, we can then determine the accuracy or error rate of our neural network using our **loss functions** and **cost functions**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98cee2-acd8-42f6-a68f-3b0e6fbf2f07",
   "metadata": {},
   "source": [
    "***Loss functions***, as explained from the 3rd notebook, is used to *measure the error rate or discrepancy between the predicted and actual values of a neural network*. By going through our training data set through forward propagation, we will have a collection of loss function output reflecting the error rates of each sample when it passes through our neural network. This will then in turn be evaluated using our ***cost functions*** to *determine the average error rate accross the set of samples*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3710a3-d95e-42fd-8203-eaa228219210",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1478114-90b2-4803-b90f-7ac397c411b7",
   "metadata": {},
   "source": [
    "Once we have determined the accuracy/error rate of our neural network through forward propagation, we now have to adjust our neural networks trainable parameters, *weights* and *biases*, to further improve our model's performance. This is achieved through ***back propagation***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500d6e8-3b89-47f3-8ac2-dc291214b9e2",
   "metadata": {},
   "source": [
    "**Back propagation** is the process in which we adjust the weights and biases of our nodes in each layer to improve the model's accuracy and performance. As explained in the 3rd notebook, *weights and biases determines the relationship between our target (y) and feature variables (x) and adds flexibility to such relationship respectively*. Overall, this provides that ***each node has varying contributions towards the prediction capabilities of our model***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f3dd4-10f3-4d64-9ee8-b961847db07a",
   "metadata": {},
   "source": [
    "Back propagation is achieved by computing the ***delta value*** for each layer of the neural netowrk starting from the output layer based from the result of our cost function, and modify the layer's weights and biases accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9cb1f-24aa-4872-b7d9-15bbe20fe906",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4c048ae-c952-4021-b388-fa1961d4066e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334df32f-54e8-4cbf-a8a9-9a8d470acd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
