{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9cc808",
   "metadata": {},
   "source": [
    "# Iris\n",
    "\n",
    "This examples demonstrates the core deep learning model building concepts using the Keras library. The Iris flower dataset is used to build the model and perform classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabf059",
   "metadata": {},
   "source": [
    "## 4.2. Prepare Input Data for Deep Learning\n",
    "\n",
    "Prepare and preprocess our input data to convert into something our neural network can read and access.\n",
    "\n",
    "1. Load data into a Pandas DataFrame\n",
    "2. Preprocess the categorical target variables\n",
    "3. Convert the DataFrame to a Numpy array\n",
    "4. Scale the feature dataset\n",
    "5. Use one-hot-encoding for the target variable\n",
    "6. Split into training and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453f74bc-b0f8-4e74-8a7d-7fa4aee5918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Python libraries for our model training\n",
    "import os\n",
    "import pandas as pandas\n",
    "import tensorflow as tensorflow\n",
    "import numpy as numpy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b8eec5-8e46-4d0e-874f-cb780115bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Sepal.Length  150 non-null    float64\n",
      " 1   Sepal.Width   150 non-null    float64\n",
      " 2   Petal.Length  150 non-null    float64\n",
      " 3   Petal.Width   150 non-null    float64\n",
      " 4   Species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load data from our CSV file into a Pandas DataFrame\n",
    "iris_data = pandas.read_csv('iris.csv')\n",
    "iris_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60491464-8a1d-4fa3-acc4-ecb411264534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess categorical target variable into numeric values\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "iris_data['Species'] = label_encoder.fit_transform(iris_data['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4dbb32-84e7-4296-b3e9-d9d6d49fada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame into Numpy Array/s\n",
    "numpy_iris = iris_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15da1cec-5183-4a33-8e26-a8a1db6a76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the feature dataset\n",
    "\n",
    "## Separate feature variables from the target variable\n",
    "x_data = numpy_iris[:,0:4]\n",
    "y_data = numpy_iris[:,4]\n",
    "\n",
    "## Create and train Scaler model fit for the given input data\n",
    "scaler_model = StandardScaler().fit(x_data)\n",
    "\n",
    "## Scale numeric feature variables\n",
    "x_data = scaler_model.transform(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05fba9b-2e02-4028-bab2-5d898a752c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one-hot encoding for categorical target variables\n",
    "y_data = tensorflow.keras.utils.to_categorical(y_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709a9da8-3f8e-4521-9ebf-6556237601a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_training_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split training and testing data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x_data, x_test, y_data, y_test \u001b[38;5;241m=\u001b[39m \u001b[43msplit_training_test\u001b[49m(x_data, y_data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_training_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Split training and testing data\n",
    "x_data, x_test, y_data, y_test = split_training_test(x_data, y_data, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a23ba2-06d6-44c5-a883-795132265c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
