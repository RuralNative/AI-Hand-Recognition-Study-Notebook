{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec3594a-344c-41d8-808e-6932a13e8146",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f2bf3-cc97-4410-bcad-94a37aab99b6",
   "metadata": {},
   "source": [
    "A **neural network architecture** refers to the *structure or layout of a neural network*. It defines how the individual nodes or layers are organized and connected to each other. The architecture *determines how the network processes and transforms input data to produce output predictions or classifications*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c3ab5-dbc0-4c6b-b3ef-9deb3f1a24c1",
   "metadata": {},
   "source": [
    "**Direction** definition is *PENDING*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a5e1a-b7c2-4ad2-acea-51927cdbb324",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49bbbb-e547-4b5e-a2ea-87a749081764",
   "metadata": {},
   "source": [
    "The **input layer** is responsible in *preprocessing and transforming the input data to its appropriate numeric representations prior* to being passed to every node in the first hidden layer. It ***does not have its own weights, biases, or activation functions***; as they only come to play inside the hidden layers. The input layer ensures that the neural network can *better handle different types of data*, *improve convergence during training*, and *enhance the overall performance of the model*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeb83b8-d0f9-4d22-a05c-6d444c9bd0b1",
   "metadata": {},
   "source": [
    "*Below are the specific and detailed steps performed by the input layer for the data preprocessing:*\n",
    "* **Data Scaling/Normalization**: This involves scaling the input data to a specific range or normalizing it to have zero mean and unit variance. Scaling the data ensures that all features have a similar scale, which can help improve the performance and convergence of the neural network during training.\n",
    "* **Data Encoding**: If the input data contains categorical variables, they may need to be encoded into numerical values. This can be done using techniques such as one-hot encoding, label encoding, or ordinal encoding, depending on the nature of the categorical variables.\n",
    "* **Missing Data Handling**: If the input data contains missing values, preprocessing in the input layer may involve handling these missing values. This can be done by imputing the missing values with a suitable strategy, such as mean imputation, median imputation, or using more advanced techniques like regression imputation or multiple imputation.\n",
    "* **Feature Selection/Extraction**: In some cases, the input data may contain a large number of features, and not all of them may be relevant for the task at hand. Preprocessing in the input layer may involve selecting a subset of the most informative features or performing feature extraction techniques, such as principal component analysis (PCA) or linear discriminant analysis (LDA), to reduce the dimensionality of the input data.\n",
    "* **Data Reshaping**: Depending on the specific requirements of the neural network architecture, the input data may need to be reshaped or restructured. For example, if the network expects a certain input shape, such as a specific number of rows and columns, the input data may need to be reshaped accordingly.gly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d9d78-6579-4b87-a162-6b592818b043",
   "metadata": {},
   "source": [
    "## Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69f31c-8dba-4d4b-bd75-b2ba66517b73",
   "metadata": {},
   "source": [
    "The **hidden layers** serves as the brain of the neural network, responsible in ***processing the data to capture and determine the complex relationships and relationship existent in them*** leading to *improved generalization and prediction capabilities in the output layer*. Each node in the individual hidden layers processes and identifies relationships between the target variable (y) and the feature variables (x), and passes their output to all nodes found in the succeeding layer, persisted with their passed weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121a15f-d0e3-4333-bf6c-0458df9f958b",
   "metadata": {},
   "source": [
    "*Below are the specific and detailed tasks performed by the hidden layer:*\n",
    "* **Weight and Bias Adjustment**: The hidden layers contribute to determining the optimal values of weights and biases. The weights control the strength of the connections between neurons, while the biases introduce an additional parameter that helps in shifting the activation function. By adjusting these parameters, the hidden layers allow the neural network to learn and adapt to the patterns in the data.\n",
    "* **Non-Linear Transformations**: The primary function of the hidden layers is to perform non-linear transformations on the inputs. Each neuron in the hidden layer applies an activation function to the weighted sum of its inputs. This introduces non-linearity into the model, enabling the neural network to capture complex relationships and patterns in the data. The hidden layers allow the network to learn and represent non-linear mappings between the input and output variables.\n",
    "* **Feature Extraction and Representation**: The hidden layers also play a crucial role in feature extraction and representation learning. As the data passes through the hidden layers, they learn to extract relevant features from the input data. Each hidden layer can learn to detect and represent different levels of abstraction and complexity. By combining these learned features, the network can make more accurate predictions in the output layer.\n",
    "* **Hierarchical Learning**: Neural networks with multiple hidden layers can learn hierarchical representations of the data. The early hidden layers learn low-level features, such as edges or corners, while the deeper hidden layers learn more abstract and high-level features. This hierarchical learning allows the network to capture intricate patterns and relationships in the data, leading to improved performance.\n",
    "* **Generalization and Prediction**: The hidden layers, along with the learned weights and biases, contribute to the network's ability to generalize and make predictions. By learning from the training data, the hidden layers adjust the weights and biases to minimize the difference between the predicted output and the actual output. This enables the network to make accurate predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bf04d-5ff7-4711-b41b-d702d77e9fd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b77f97ac-58a8-4e6c-99c4-2134b4abce5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "968592de-2589-4341-aab9-42a9d0a4c840",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efebfc1-75db-4883-82e5-01cd995e24bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca95a622-9512-46b5-a792-f0c8a863397c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "641d28dd-b2bc-4c6f-91d9-05fea7f959b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e97cbba-2358-4cea-af02-fb2476556d9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05842591-f94b-44d3-93f2-a2f937bdad0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d29610f2-83a8-4b9b-ba63-6e6b57daa3b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2605c1f-f459-4165-88e7-f0db66cec721",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c6ee4d7-e78b-40a3-840f-60f7928943d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed06a4d-7baa-48a8-93e6-ae71809b0090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
